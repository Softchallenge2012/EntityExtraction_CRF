{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e969108-d8e3-4989-b8c0-520c2a293937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucy/opt/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea219ca-aa74-48db-bc2e-bc8625a01c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414177ac-9ec8-46a3-b7d1-94d9b6fcbd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ce7079fc-21d8-49f8-82de-22ad1fd750ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab_names = pd.read_csv('/home/jovyan/work/NER-Test/data/labtest_dictionary_v01.csv')\n",
    "# lab_names = lab_names['test'].tolist()\n",
    "\n",
    "lab_names = pd.read_csv('data/labtest_dictionary_train_cleaned_v04.csv')\n",
    "lab_names = lab_names['test name'].tolist()\n",
    "\n",
    "VALUES = pd.read_csv('data/labtest_additional_values_v02.csv')\n",
    "VALUES = VALUES['values'].tolist()\n",
    "VALUES = [str(v).upper for v in VALUES]\n",
    "\n",
    "UNITS = pd.read_csv('data/labtest_units_v04.csv')\n",
    "UNITS = UNITS['units'].tolist()\n",
    "UNITS = [str(u).upper() for u in UNITS]\n",
    "UNITS = list(set(UNITS))\n",
    "\n",
    "ABNORMAL = pd.read_csv('data/labtest_abnormal.csv')\n",
    "ABNORMAL = ABNORMAL['abnormal flag'].tolist()\n",
    "ABNORMAL = [str(v).upper() for v in ABNORMAL]\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = str(sent[i][0])\n",
    "    postag = str(sent[i][1])\n",
    "    \n",
    "    # values\n",
    "    # units\n",
    "    # references\n",
    "    # abnormal flags\n",
    "    # datetime\n",
    "    \n",
    "    isTestName = True if len([token for w in lab_names for token in str(w).upper().split(' ') if token.find(word)>-1])>0 else False\n",
    "    isValue = True if re.match(r'\\d*.?\\d+|[0-9\\.\\-\\s\\>\\<\\=]+', word) or word in VALUES else False    \n",
    "    isUnit = True if word.upper() in UNITS else False\n",
    "    isDecimal = True if re.findall(r'(\\.)', word) and word.isdigit() else False\n",
    "    isDigit = True if word.isdigit() else False\n",
    "    isRange = True if re.match(r'\\([0-9\\.\\-\\s\\>\\<\\=]+\\)', word) else False # match parenthesis+digits\n",
    "    isAbnormal = True if word.upper() in ABNORMAL else False\n",
    "    isDatetime = True if re.match(r'\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}[\\s]+\\d{1,2}:\\d{1,2}[:\\d+]*|\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}|\\d{1,2}:\\d{1,2}[:\\d+]*', str(word)) else False\n",
    "    #isRange = True if re.match(r'\\(\\d+-\\d+\\)', word) else False\n",
    "    \n",
    "    features = {\n",
    "        'word.isTestName()': word if isTestName else '',\n",
    "        'word.isValue()': word if isValue else '',\n",
    "        'word.isUnit()': word if isUnit else '',\n",
    "        'word.isDecimal()': word if isDecimal else '',\n",
    "        'word.isDigit()': word if isDigit else '',\n",
    "        'word.isRange()': word if isRange else '',\n",
    "        'word.isAbnormal()': word if isAbnormal else '',\n",
    "        #'word.isDatetime()': word if isDatetime else '',\n",
    "        #'postag': postag,\n",
    "        #'postag[:2]': postag[:2],\n",
    "    }\n",
    "    try:\n",
    "        if i > 0:\n",
    "            word1 = sent[i - 1][0]\n",
    "            postag1 = sent[i - 1][1]\n",
    "\n",
    "            isTestName1 = True if len([token for w in lab_names for token in str(w).upper().split(' ') if token.find(word1)>-1])>0 else False\n",
    "            isValue1 = True if re.match(r'\\d*.?\\d+|[0-9\\.\\-\\s\\>\\<\\=]+', word1)  or word in VALUES else False    \n",
    "            isUnit1 = True if word1.upper() in UNITS else False\n",
    "            isDecimal1 = True if re.findall(r'(\\.)', word1) and word.isdigit() else False\n",
    "            isDigit1 = True if word1.isdigit() else False\n",
    "            isRange1 = True if re.match(r'\\([0-9\\.\\-\\s\\>\\<\\=]+\\)', word1) else False # match parenthesis+digits\n",
    "            isAbnormal1 = True if word1.upper() in ABNORMAL else False\n",
    "            isDatetime1 = True if re.match(r'\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}[\\s]+\\d{1,2}:\\d{1,2}[:\\d+]*|\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}|\\d{1,2}:\\d{1,2}[:\\d+]*', str(word1)) else False\n",
    "\n",
    "\n",
    "            features.update({\n",
    "                '-1word.isTestName()': word1 if isTestName1 else '',\n",
    "                '-1word.isValue()': word1 if isValue1 else '',\n",
    "                '-1word.isUnit()': word1 if isUnit1 else '',\n",
    "                '-1word.isDecimal()': word1 if isDecimal1 else '',\n",
    "                '-1word.isDigit()': word1 if isDigit1 else '',\n",
    "                '-1word.isRange()': word1 if isRange1 else '',\n",
    "                '-1word.isAbnormal()': word1 if isAbnormal1 else '',\n",
    "                #'-1word.isDatetime()': word1 if isDatetime1 else '',\n",
    "                #'-1postag': postag1,\n",
    "                #'-1postag[:2]': postag1[:2],\n",
    "            })\n",
    "        else:\n",
    "            #features['BOS'] = True\n",
    "            features.update({\n",
    "                'BOS': word\n",
    "            })\n",
    "\n",
    "        if i < len(sent) - 1:\n",
    "            word1 = sent[i + 1][0]\n",
    "            postag1 = sent[i + 1][1]\n",
    "\n",
    "            isTestName1 = True if len([token for w in lab_names for token in str(w).upper().split(' ') if token.find(word1)>-1])>0 else False\n",
    "            isValue1 = True if re.match(r'\\d*.?\\d+|[0-9\\.\\-\\s\\>\\<\\=]+', word1)  or word in VALUES else False    \n",
    "            isUnit1 = True if word1.upper() in UNITS else False\n",
    "            isDecimal1 = True if re.findall(r'(\\.)', word1) and word.isdigit() else False\n",
    "            isDigit1 = True if word1.isdigit() else False\n",
    "            isRange1 = True if re.match(r'\\([0-9\\.\\-\\s\\>\\<\\=]+\\)', word1) else False # match parenthesis+digits\n",
    "            isAbnormal1 = True if word1.upper() in ABNORMAL else False\n",
    "            isDatetime1 = True if re.match(r'\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}[\\s]+\\d{1,2}:\\d{1,2}[:\\d+]*|\\d{1,2}\\/\\d{1,2}\\/\\d{2,4}|\\d{1,2}:\\d{1,2}[:\\d+]*', str(word1)) else False\n",
    "\n",
    "\n",
    "            features.update({\n",
    "                '+1word.isTestName()': word1 if isTestName1 else '',\n",
    "                '+1word.isValue()': word1 if isValue1 else '',\n",
    "                '+1word.isUnit()': word1 if isUnit1 else '',\n",
    "                '+1word.isDecimal()': word1 if isDecimal1 else '',\n",
    "                '+1word.isDigit()': word1 if isDigit1 else '',\n",
    "                '+1word.isRange()': word1 if isRange1 else '',\n",
    "                '+1word.isAbnormal()': word1 if isAbnormal1 else '',\n",
    "                #'+1word.isDatetime()': word1 if isDatetime1 else '',\n",
    "                #'+1postag': postag1,\n",
    "                #'+1postag[:2]': postag1[:2],\n",
    "            })\n",
    "        else:\n",
    "            #features['EOS'] = True        \n",
    "            features.update({\n",
    "                'EOS': word\n",
    "            })\n",
    "    except:\n",
    "        print(sent,word)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c1502-2e20-4c3c-9865-3dd3a64200d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a83ac759-1886-46e4-97cf-ad91ceee1bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_dup(s):\n",
    "    s = str(s).upper()\n",
    "    terms = re.findall(r'\\([s]*[A-Z0-9\\s]+[\\s]*\\)',s)    \n",
    "    for t in terms:\n",
    "        tt = re.sub(r'[^A-Z0-9\\s]','',t)\n",
    "        #print(tt)\n",
    "        if len(tt)>1:\n",
    "            if len([ w for w in s.replace(t,'').split(' ') if w in re.split(r'[\\s]+', str(tt).strip())])>0 :\n",
    "                #print(t)\n",
    "                s =  s.replace(t,'')\n",
    "    return s\n",
    "\n",
    "def clean_results(v):\n",
    "    v = re.sub(r'\\\"','',str(v).strip())            \n",
    "    v = re.sub(r'[^A-Z0-9\\:\\/\\-\\.\\(\\)]',' ',str(v).strip().upper())\n",
    "    #v = re.sub(r'([^\\s]+)([\\s]*-[\\s]*)([^\\s]+)',r'\\1-\\2',str(v).strip().upper())\n",
    "    v = re.sub(r'([^\\s]+)([\\s]*-[\\s]*)([^\\s]+)',r'\\1-\\3',str(v).strip().upper())\n",
    "    v = replace_dup(v)\n",
    "    v = re.sub(r'(\\()[\\s]+(\\w+)[\\s]*(\\))',r'\\1\\2\\3',v)\n",
    "    v = ','.join(re.split(r'[\\s]+',str(v)))\n",
    "    v = re.sub(r'(\\d{1,2}\\/\\d{1,2}\\/\\d{2,4})(,)(\\d{1,2}:\\d{1,2}[:\\d+]*)','',v)\n",
    "    return v\n",
    "\n",
    "def verify_tagged_set(df_sent):\n",
    "    # raw, lab name, value, unit, reference, abnormal flag, datetime\n",
    "    df_sent = df_sent.fillna(value='')\n",
    "\n",
    "    filtered = []\n",
    "    for key, values in df_sent.iterrows():\n",
    "        arow = []\n",
    "        values = [re.sub(r'[\\\"|\\'\\[\\]]','',str(w)) for w in values]\n",
    "        #values[0] = v\n",
    "        #print(v)\n",
    "        tags = []\n",
    "        #print(values)\n",
    "        for aword in re.split(r',',str(values[0])):\n",
    "            atag = ''\n",
    "            apos = ''\n",
    "            if len(values[2])>0 and len([w for w in values[2].split(' ') if str(w).upper() == str(aword).upper()]) >0:\n",
    "                atag = 'VALUE'\n",
    "                apos = 'CD'\n",
    "                tags.append((aword, apos, atag))\n",
    "            elif len(values[3])>0 and len([w for w in values[3].split(',') if str(aword).upper()==str(w).upper()]) >0:\n",
    "                atag = 'UNIT'\n",
    "                apos = 'CD'\n",
    "                tags.append((aword, apos, atag))\n",
    "            elif len(re.sub(r'([^\\s]+)([\\s]*-[\\s]*)([^\\s]+)',r'\\1-\\3',str(values[4]).strip().upper()))>0 and \\\n",
    "            len([w for w in re.sub(r'([^\\s]+)([\\s]*-[\\s]*)([^\\s]+)',r'\\1-\\3',str(values[4]).strip().upper()).split(' ') \\\n",
    "                 if str(aword).upper()==str(w).upper()]) >0:\n",
    "                atag = 'REF'\n",
    "                apos = 'CD'\n",
    "                tags.append((aword, apos, atag))\n",
    "            elif len(values[5])>0 and len([w for w in values[5].split(',') if str(aword).upper()==str(w).upper() or str(aword).upper()=='('+str(w)+')'.upper()]) >0:\n",
    "                atag = 'ABNORM'\n",
    "                apos = 'NN'\n",
    "                tags.append((aword, apos, atag))\n",
    "            elif len(values[6])>0 and len([w for w in values[6].split(',') if str(aword).upper()==str(w).upper()]) >0:\n",
    "                atag = 'DTIME'\n",
    "                apos = 'CD'\n",
    "                tags.append((aword, apos, atag))\n",
    "            else:\n",
    "                atag = 'O'                \n",
    "                wordlist = nltk.pos_tag(nltk.word_tokenize(aword)) \n",
    "                for i in range(len(wordlist)):\n",
    "                    aword, apos = wordlist[i]\n",
    "                    tags.append((aword, apos, atag))\n",
    "        filtered.append(pd.DataFrame(tags))\n",
    "\n",
    "    tagged_list = []\n",
    "    for key, values in enumerate(filtered):\n",
    "        for k,atagged in values.iterrows():\n",
    "            tagged_list.append([key, atagged[0], atagged[1], atagged[2]])\n",
    "    tagged_df = pd.DataFrame(tagged_list)\n",
    "    tagged_df = tagged_df.rename(columns={0: 'Sentence #', 1: 'Word', 2: 'POS', 3: 'Tag'})\n",
    "    tagged_df.shape\n",
    "    \n",
    "    return tagged_df\n",
    "\n",
    "def make_tagged_set(df_sent):\n",
    "    # raw, lab name, value, unit, reference, abnormal flag, datetime\n",
    "    df_sent = df_sent.fillna(value='')\n",
    "    filtered = []\n",
    "    for key, values in df_sent.iterrows():\n",
    "        arow = []\n",
    "        tags = []\n",
    "        #print(values)\n",
    "        if len(re.split(r',[\\s]*',re.sub(r'\\[|\\]','',values[0])))!= \\\n",
    "            len(re.split(r',[\\s]*',re.sub(r'\\[|\\]','',values[1]))):\n",
    "             print(values,len(values[0].split(',')), len(values[1].split(',')))\n",
    "        else:    \n",
    "            for aword, atag in zip(re.split(r',[\\s]*',re.sub(r'\\[|\\]','',values[0])), \\\n",
    "                                   re.split(r',[\\s]*',re.sub(r'\\[|\\]','',values[1]))):\n",
    "\n",
    "                apos = ''\n",
    "                if atag == 'VALUE':                \n",
    "                    apos = 'CD'\n",
    "                    tags.append((aword, apos, atag))\n",
    "                elif atag == 'UNIT':                \n",
    "                    apos = 'CD'\n",
    "                    tags.append((aword, apos, atag))\n",
    "                elif atag == 'REF':                \n",
    "                    apos = 'CD'\n",
    "                    tags.append((aword, apos, atag))\n",
    "                elif atag == 'ABNORM':                \n",
    "                    apos = 'NN'\n",
    "                    tags.append((aword, apos, atag))\n",
    "                elif atag == 'DTIME':                \n",
    "                    apos = 'CD'\n",
    "                    tags.append((aword, apos, atag))\n",
    "                else:\n",
    "                    aword = re.sub(r'[\\(|\\)]','',aword)\n",
    "                    wordlist = nltk.pos_tag(nltk.word_tokenize(aword)) \n",
    "                    for i in range(len(wordlist)):\n",
    "                        aword, apos = wordlist[i]\n",
    "                        tags.append((aword, apos, atag))\n",
    "            filtered.append(pd.DataFrame(tags))\n",
    "    print(len(filtered))\n",
    "    tagged_list = []\n",
    "    for key, values in enumerate(filtered):\n",
    "        for k,atagged in values.iterrows():\n",
    "            tagged_list.append([key, atagged[0], atagged[1], atagged[2]])\n",
    "    tagged_df = pd.DataFrame(tagged_list)\n",
    "    tagged_df = tagged_df.rename(columns={0: 'Sentence #', 1: 'Word', 2: 'POS', 3: 'Tag'})\n",
    "    print(tagged_df.shape)\n",
    "    \n",
    "    return tagged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b6637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(s):\n",
    "    s = re.split(r'[\\([s]*[A-Z0-9\\s]+[\\s]*\\)]|[\\s]+', str(s).upper())    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5dd0d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 17.9 ms, total: 146 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_test_data = pd.read_csv('data/train/train_round123_v31_structured_v14_label_test_tagged.csv')\n",
    "# df_test_data['lab_result_1'] = df_test_data['lab_result_0']\n",
    "# df_test_data['lab_result_0'] = df_test_data['lab_result_0'].apply(clean_results)\n",
    "#test_tagged = make_tagged_set(df_test_data[['lab_result_0','true_label']])\n",
    "df_test_data['parsed'] = df_test_data['lab_result_1'].apply(preprocessing)\n",
    "df_test_data['check'] = df_test_data[['lab_result_0','parsed']].apply(lambda s: re.split(r'[\\s]*,[\\s]*',re.sub(r'[\\[\\]\\'\\\"]','',str(s[0])))==re.split(r'[\\s]*,[\\s]*',re.sub(r'[\\[\\]\\'\\\"]','',str(s[1]))), axis=1)\n",
    "df_test_data.to_csv('data/test/train_round123_v31_structured_v14_preprocessing.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a508b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2fa2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d714c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81af89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a81aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40107cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.11.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
